{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import dnnlib\n",
    "import legacy\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import math\n",
    "from torchvision.utils import make_grid\n",
    "import pandas as pd\n",
    "import PIL \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lrs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = os.getcwd().replace(\"\\\\\", \"/\") + '/MLP_dataset_cpu/Nueva carpeta/'\n",
    "\n",
    "with open(path + \"clip_embeddings_cpu.pkl\", 'rb') as f:\n",
    "    clip_embeddings = pickle.load(f)\n",
    "\n",
    "with open(path + \"latent_vectors_cpu.pkl\", 'rb') as f:\n",
    "    latent_vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2051,  0.0146, -0.2891,  ...,  0.0631,  0.2717,  0.1092],\n",
      "        [-0.0046, -0.3018, -0.3093,  ...,  0.3403,  0.2842, -0.0611],\n",
      "        [ 0.0673, -0.3384, -0.5674,  ...,  0.4553,  0.3196, -0.3755],\n",
      "        ...,\n",
      "        [-0.1978, -0.1085, -0.3662,  ...,  0.1799, -0.2476,  0.0675],\n",
      "        [ 0.2529,  0.2294,  0.2333,  ...,  0.2362, -0.2019,  0.4663],\n",
      "        [ 0.4688, -0.2537, -0.3467,  ...,  0.8003, -0.1028,  0.0372]],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(clip_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MLP_path = os.getcwd().replace(\"\\\\\", \"/\") + '/MLP_dataset_std1/'\n",
    "\n",
    "clip_embedding_ = []\n",
    "latent_vector_ = []\n",
    "for i in range(0, len(os.listdir(MLP_path))//2):\n",
    "    with open(MLP_path+'clip_embeddings_' + str(i) + '.pkl', 'rb') as f:\n",
    "        clip_embedding_.append(pickle.load(f))\n",
    "    with open(MLP_path+'latent_vectors_' + str(i) + '.pkl', 'rb') as f:\n",
    "        latent_vector_.append(pickle.load(f))\n",
    "\n",
    "# put all the embeddings and latent vectors in one tensor\n",
    "clip_embedding = torch.cat(clip_embedding_)\n",
    "latent_vector = torch.cat(latent_vector_)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(clip_embedding, latent_vector)\n",
    "\n",
    "# split the dataset into train, validation and test sets\n",
    "train, val= torch.utils.data.random_split(dataset, [int(len(dataset)*0.85), int(len(dataset)*0.15)])\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=48, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=48, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an auoenconder class \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_hidden=3):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = 256\n",
    "        self = 256\n",
    "        self.n_hidden = n_hidden\n",
    "        self.layers = nn.ModuleList()\n",
    "        if n_hidden==0:\n",
    "            self.layers.append(nn.Linear(self.input_dim, self.output_dim))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(self.input_dim, self.hidden_dim))\n",
    "            for i in range(self.n_hidden-1):\n",
    "                self.layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "            self.layers.append(nn.Linear(self.hidden_dim, self.output_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_hidden):\n",
    "            x = self.relu(self.layers[i](x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Autoencoder Definition\n",
      "Autoencoder(\n",
      "  (encoder): Encoder(\n",
      "    (layer1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (layer2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (layer3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layer1): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (layer2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (layer3): Linear(in_features=256, out_features=512, bias=True)\n",
      "  )\n",
      "  (loss): MSELoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# reduce the dimensionality of the clip_embeddings using autoencoder\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.layer1 = nn.Linear(512,256)\n",
    "    self.layer2 = nn.Linear(256,128)\n",
    "    self.layer3 = nn.Linear(128, 64)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    out = self.layer1(x)\n",
    "    out = self.layer2(out)\n",
    "    return self.layer3(out)\n",
    "    \n",
    "# Decoder definition with a fully-connected layer and 3 BN-ReLU-COnv blocks and \n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.layer1 = nn.Linear(64,128)\n",
    "    self.layer2 = nn.Linear(128,256)\n",
    "    self.layer3 = nn.Linear(256,512)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        return self.layer3(out)\n",
    "    \n",
    "  \n",
    "class Autoencoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    # Initialize the encoder and decoder using a dimensionality out_features for the vector z\n",
    "    self.encoder = Encoder()\n",
    "    self.decoder = Decoder()\n",
    "    self.loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "  def forward(self,x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return encoded, decoded\n",
    "  \n",
    "  def backward(self, decoded, x):\n",
    "    # Reconstruction loss\n",
    "    recon_loss = self.loss(decoded, x)\n",
    "    return recon_loss\n",
    "\n",
    "# Print summary of the mode\n",
    "print('MNIST Autoencoder Definition')\n",
    "autoencoder = Autoencoder()\n",
    "print(autoencoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the autoencoder\n",
    "def train(model, train_loader, optimizer, epochs, log_interval=100):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(epochs):\n",
    "        loss_epoch = []\n",
    "        for data in train_loader:\n",
    "            data = data\n",
    "            print(data.shape)\n",
    "            print(type(data))\n",
    "            print(data)\n",
    "            optimizer.zero_grad()\n",
    "            _, decoded = model(data)\n",
    "            loss = model.backward(decoded, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_epoch.append(loss.item())\n",
    "        batch_loss = np.mean(loss_epoch)\n",
    "        losses.append(batch_loss)\n",
    "        print('Epoch: {} \\tLoss: {:.6f}'.format(epoch, batch_loss))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10040, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = torch.utils.data.TensorDataset(clip_embedding)\n",
    "len(dataset), len(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_dataset = torch.utils.data.TensorDataset(clip_embedding)\n",
    "train_loader = torch.utils.data.DataLoader(clip_dataset, batch_size=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m clip_autoencoder\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(clip_autoencoder\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m losses \u001b[39m=\u001b[39m train(clip_autoencoder, train_loader, optimizer, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, epochs, log_interval)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m      9\u001b[0m     data \u001b[39m=\u001b[39m data\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39;49mshape)\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(data))\n\u001b[0;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "clip_autoencoder = Autoencoder()\n",
    "clip_autoencoder.cuda()\n",
    "clip_autoencoder.train()\n",
    "optimizer = torch.optim.Adam(clip_autoencoder.parameters(), lr=1e-3)\n",
    "losses = train(clip_autoencoder, train_loader, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "albertkernel",
   "language": "python",
   "name": "albertkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
